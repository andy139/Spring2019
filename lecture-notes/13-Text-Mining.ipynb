{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Dataset Challenge \n",
    "\n",
    "\n",
    "Yelp challenges students to use their data in innovative ways and break ground in research. There is a myriad of deep, machine learning questions to tackle with this rich dataset:\n",
    "\n",
    "- How well can you guess a review's rating from its text alone? \n",
    "- Can you take all of the reviews of a business and predict when it will be the most busy, or when the business is open? \n",
    "- Can you predict if a business is good for kids? Has WiFi? Has Parking? \n",
    "- What makes a review useful, funny, or cool? \n",
    "- Can you figure out which business a user is likely to review next? \n",
    "- How much of a business's success is really just location, location, location? \n",
    "- What businesses deserve their own subcategory (i.e., Szechuan or Hunan versus just “Chinese restaurants”), and can you learn this from the review text?\n",
    "- What are the differences between the cities in the dataset?\n",
    "\n",
    "See some of the [past winners](https://www.yelp.com/dataset/challenge/winners) and hundreds of [academic papers](https://scholar.google.com/scholar?q=citation%3A+Yelp+Dataset&btnG=&hl=en&as_sdt=0%2C5) written using the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Yelp Dataset JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data: https://www.yelp.com/dataset/download. \n",
    "\n",
    "Each file is composed of a single object type, one JSON-object per-line.\n",
    "\n",
    "- `business.json`: contains business data including location data, attributes, and categories.\n",
    "- `review.json`: Contains full review text data including the `user_id` that wrote the review and the `business_id` the review is written for.\n",
    "- `user.json`: User data including the user's friend mapping and all the metadata associated with the user.\n",
    "- `checkin.json`: Checkins on a business.\n",
    "- `tip.json`: Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.\n",
    "- `photos` (from the photos auxiliary file): This file is formatted as a JSON list of objects.\n",
    "\n",
    "Take a look at some examples to get you started: https://github.com/Yelp/dataset-examples.\n",
    "\n",
    "The dataset consist of very large json files. Here are the file sizes:\n",
    "\n",
    "\n",
    "| File name                               | Size |\n",
    "|-----------------------------------------|------|\n",
    "| business.json                           | 139M |\n",
    "| checkin.json                            |  61M |\n",
    "| Dataset_Challenge_Dataset_Agreement.pdf |  98K |\n",
    "| photos.json                             |  26M |\n",
    "| review.json                             | 4.0G |\n",
    "| tip.json                                | 189M |\n",
    "| user.json                               | 1.8G |\n",
    "| Yelp_Dataset_Challenge_Round_11.pdf     | 111K |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data loading\n",
    "\n",
    "\n",
    "## Business data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading `business.json` in order to filter out business only with a sufficiently large number of reviews.\n",
    "\n",
    "The file `business.json` was previously saved in a Google drive so that we can `wget` from the shared link. The latter process is done using the bash [script below](https://gist.github.com/iamtekeste/3cdfd0366ebfd2c0d805#gistcomment-2359248):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -nv https://ucsb.box.com/shared/static/c5ulwkcaka7hych2ou8bjwz7p8gcvxyw.json -O business.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\":\"1SWheh84yJXfytovILXOAQ\",\"name\":\"Arizona Biltmore Golf Club\",\"address\":\"2818 E Camino Acequia Drive\",\"city\":\"Phoenix\",\"state\":\"AZ\",\"postal_code\":\"85016\",\"latitude\":33.5221425,\"longitude\":-112.0184807,\"stars\":3.0,\"review_count\":5,\"is_open\":0,\"attributes\":{\"GoodForKids\":\"False\"},\"categories\":\"Golf, Active Life\",\"hours\":null}\r\n"
     ]
    }
   ],
   "source": [
    "! head -n1 business.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jq`\n",
    "\n",
    "To avoid reading `business.json` (a very large file) into memory in order to select businesses with high number of reviews, we take avantage of the fact that business information consists of several JSON objects and use a command line tool called [jq](https://stedolan.github.io/jq/) in order to slice, filter, map and transform JSON data with the same ease that `sed`, `awk`, `grep`.\n",
    "\n",
    "Also, `jq` is useful for processing large files using the [streaming parser](https://github.com/stedolan/jq/wiki/FAQ#streaming-json-parser). Streaming parser does not load the whole file into memory.\n",
    "\n",
    "`jq` doesn't need to be installed. One executable binary contains everything you need. Find the right executable for your OS here: https://stedolan.github.io/jq/download/\n",
    "\n",
    "The one for linux is here: https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-23 16:06:36 URL:https://github-production-release-asset-2e65be.s3.amazonaws.com/5101141/6387d980-de1f-11e8-8d3e-4455415aa408?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190523%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190523T160635Z&X-Amz-Expires=300&X-Amz-Signature=0b314b76b1a332188a34476acc4d5d991a7b1923320faa72389f0288c5963028&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Djq-linux64&response-content-type=application%2Foctet-stream [3953824/3953824] -> \"/opt/conda/bin/jq\" [1]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm /opt/conda/bin/jq\n",
    "wget -nc -nv https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -O /opt/conda/bin/jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash does not know that `jq` is an executable file. You have to tell it that users(`u`) can execute(`x`) the file `jq` with `chmod` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 3.8M Nov  2  2018 /opt/conda/bin/jq\n",
      "-rwxr--r-- 1 jovyan users 3.8M Nov  2  2018 /opt/conda/bin/jq\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /opt/conda/bin/jq    ## showing jq is not executable\n",
    "! chmod u+x /opt/conda/bin/jq  ## make jq executable (this command doesn't print anything)\n",
    "! ls -alh /opt/conda/bin/jq    ## showing jq is now executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jq - commandline JSON processor [version 1.6]\r\n",
      "\r\n",
      "Usage:\tjq [options] <jq filter> [file...]\r\n",
      "\tjq [options] --args <jq filter> [strings...]\r\n",
      "\tjq [options] --jsonargs <jq filter> [JSON_TEXTS...]\r\n",
      "\r\n",
      "jq is a tool for processing JSON inputs, applying the given filter to\r\n",
      "its JSON text inputs and producing the filter's results as JSON on\r\n",
      "standard output.\r\n",
      "\r\n",
      "The simplest filter is ., which copies jq's input to its output\r\n",
      "unmodified (except for formatting, but note that IEEE754 is used\r\n",
      "for number representation internally, with all that that implies).\r\n",
      "\r\n",
      "For more advanced filters see the jq(1) manpage (\"man jq\")\r\n",
      "and/or https://stedolan.github.io/jq\r\n",
      "\r\n",
      "Example:\r\n",
      "\r\n",
      "\t$ echo '{\"foo\": 0}' | jq .\r\n",
      "\t{\r\n",
      "\t\t\"foo\": 0\r\n",
      "\t}\r\n",
      "\r\n",
      "For a listing of options, use jq --help.\r\n"
     ]
    }
   ],
   "source": [
    "! jq ## run jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find some `jq` basic examples [here](https://robots.thoughtbot.com/jq-is-sed-for-json). Another very useful resource to text `jq` commands on-line: https://jqplay.org/.\n",
    "\n",
    "The `jq` line below executes the following: \n",
    "\n",
    "1. [Selects a subset of keys from an object\n",
    "](https://stackoverflow.com/questions/29518137/jq-selecting-a-subset-of-keys-from-an-object?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) using `with_entries()`. \n",
    "\n",
    "2. [Filters out the JSON lines according to a specific criteria](http://bigdatums.net/2016/11/16/filter-json-records-by-value-with-jq/) using `select()`.\n",
    "\n",
    "3. [More examples here](https://stedolan.github.io/jq/tutorial/)\n",
    "\n",
    "Futheremore, we use the option `jq --compact-output` (or `-c`) to get each object on a newline. \n",
    "\n",
    "Notice that in analogy to other bash commands, we can use the pipe operator `|` to use the output of one commands as the input of the subsequent one. \n",
    "\n",
    "The output is also re-directed  into `train_business.json` by using `>`.\n",
    "\n",
    "\n",
    "`business.json` contains the field `review_count` that  gives the number of reviews provided by Yelp users for any given business. We will start filtering out all businesses with at least **500 reviews** minimum number of reviews to conduct our analysis. We moreover select other fields of interest such as `business_id`, `state`, `city`, `categories` and `attributes`. \n",
    "\n",
    "Pretty-print first business with `jq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1SWheh84yJXfytovILXOAQ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Arizona Biltmore Golf Club\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"address\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2818 E Camino Acequia Drive\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"city\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Phoenix\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"state\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"AZ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"postal_code\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"85016\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"latitude\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m33.5221425\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"longitude\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m-112.0184807\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"stars\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m5\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"is_open\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"attributes\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\r\n",
      "    \u001b[0m\u001b[34;1m\"GoodForKids\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"False\"\u001b[0m\u001b[1;39m\r\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"categories\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Golf, Active Life\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"hours\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;30mnull\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! head -n1 business.json | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "A filter subsets attributes. Working with a few entries is simple with `head` and piping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1SWheh84yJXfytovILXOAQ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m5\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"QXAEGFB4oINsVuTFxEYKFQ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m128\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"gnKjwL_1w79qoiV3IC_xQQ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m170\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"xvX2CttrVhyG2z1dFg_0xw\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"HhyxOkGAM07SRYtlQ4wMFQ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m4\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! head -n5 business.json \\\n",
    "    | jq 'with_entries(select([.key] | inside([\"business_id\", \"review_count\"])))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also subset by values of `review_count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"QXAEGFB4oINsVuTFxEYKFQ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m128\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n",
      "\u001b[1;39m{\r\n",
      "  \u001b[0m\u001b[34;1m\"business_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"gnKjwL_1w79qoiV3IC_xQQ\"\u001b[0m\u001b[1;39m,\r\n",
      "  \u001b[0m\u001b[34;1m\"review_count\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m170\u001b[0m\u001b[1;39m\r\n",
      "\u001b[1;39m}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! head -n5 business.json \\\n",
    "    | jq 'with_entries(select([.key] | inside([\"business_id\", \"review_count\"]))) \\\n",
    "          | select(.review_count>20)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process the full data by making desired changes, and save to `train_business.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes some time to run\n",
    "! jq -c 'with_entries( \\\n",
    "            select([.key] | inside([\"business_id\",\"state\",\"city\",\"categories\",\"attributes\",\"review_count\"])) \\\n",
    "        ) | select(.review_count>500)' business.json > train_business.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263 train_business.json\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l train_business.json # Number of businesses with at least 500 reviews (1263)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now read `train_business.json` [into python](https://stackoverflow.com/questions/46790390/how-to-read-a-large-json-in-pandas), keeping in mind that the latter is a JSON Lines object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('train_business.json') as json_file:      \n",
    "    data_business = json_file.readlines()\n",
    "    data_business = list(map(json.loads, data_business)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our training data we will consider only 100 randomly chosen businesses from the subset of businesses with at least 500 reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_business = [data_business[index] for index \\\n",
    "                  in np.random.randint(0,len(data_business),size=100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review data\n",
    "\n",
    "We now collect the actual reviews corresponding to the 100 randomly chosen businesses with at least 500 reviews appearing in `train_business`. \n",
    "\n",
    "After trying doing so using `jq` (which would require to filter according to a list of `business_id`), I found that the most efficient way to do so is to read `review.json` (which contains ~5M JSON Lines) into python and then perform list comprehension to select the reviews for the businesses in question. The resulting *list* (named `train_review`) is serialized as a pkl object.\n",
    "\n",
    "The above procedure is included in `read_reviews.py`.\n",
    "\n",
    "Note that other approaches (including using python),\n",
    "* Read into python environment for processing\n",
    "* Use `jq` [stream parser](https://github.com/stedolan/jq/wiki/FAQ#streaming-json-parser)?\n",
    "* Use `grep` to search for business ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import pickle\r\n",
      "\r\n",
      "## read train_business.json \r\n",
      "with open('train_business.json') as json_file:      \r\n",
      "    data_business = json_file.readlines()\r\n",
      "    data_business = list(map(json.loads, data_business)) \r\n",
      "\r\n",
      "## pick 100 businesses at random\r\n",
      "train_business = [data_business[index] for index \\\r\n",
      "                  in np.random.randint(0, len(data_business), size=100)]\r\n",
      "\r\n",
      "## read train_business.json \r\n",
      "with open('review.json') as json_file:      \r\n",
      "    data_review = json_file.readlines()\r\n",
      "    data_review = list(map(json.loads, data_review)) \r\n",
      "    \r\n",
      "len(data_review) \r\n",
      "\r\n",
      "business100 = [business[\"business_id\"] for business in train_business]\r\n",
      "\r\n",
      "# add to train_review if the business_id is found in business100\r\n",
      "train_review = [review for review in data_review \\\r\n",
      "                if review['business_id'] in business100]\r\n",
      "\r\n",
      "pickle.dump(train_review, open('train_review.pkl','wb'))"
     ]
    }
   ],
   "source": [
    "! cat read_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Mining on Yelp Dataset\n",
    "\n",
    "The main resource that we will use for performing text mining on Yelp reviews text will be [Text Analytics with Python](https://github.com/dipanjanS/text-analytics-with-python).\n",
    "\n",
    "[An electronic copy of the book](https://ucsb-primo.hosted.exlibrisgroup.com/primo-explore/fulldisplay?docid=TN_springer_s978-1-4842-2388-8_427287&context=PC&vid=UCSB&lang=en_US&search_scope=default_scope&adaptor=primo_central_multiple_fe&tab=default_tab&query=any,contains,text%20analytics%20with%20python) is available through the library.\n",
    "\n",
    "We will start by downloading the auxiliary functions  which are part of the textbook repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc -nv https://raw.githubusercontent.com/dipanjanS/text-analytics-with-python/master/Old-First-Edition/source_code/Ch04_Text_Classification/feature_extractors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc -nv https://ucsb.box.com/shared/static/ne0i3no3ep9z8rjtob9ywm4uhtfrmeyn.pkl -O train_review.pkl\n",
    "\n",
    "import pickle\n",
    "train_review = pickle.load(open('train_review.pkl', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': 'kQAHUQ89fIZVCf1cFKDE_Q',\n",
       " 'user_id': 'dL1qnksdX2LW4DbNI9XNoA',\n",
       " 'business_id': 'XXW_OFaYQkkGOGniujZFHg',\n",
       " 'stars': 4.0,\n",
       " 'useful': 0,\n",
       " 'funny': 0,\n",
       " 'cool': 0,\n",
       " 'text': '\"Good morning, cocktails for you?\" \\nWait...what? Oh...it\\'s Vegas!\\n\\nDining here, you best not be dieting because this place is literally the definition of excess, but in a good way. I\\'m a sucker for benedicts so that was awesome. \\nService was really great too and the staff was so welcoming. It was our first stop just after landing so really appreciate the service.\\n\\nBack in Hawaii this reminds me of Zippys or Anna Millers - that home feeling. Prices are a bit high, but for what you get it\\'s totally worth it. Will remember this place if I ever return to Vegas in the future.',\n",
       " 'date': '2018-02-23 23:07:48'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "review_corpus = [text['text'] for text in train_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example (10 reviews)\n",
    "text = review_corpus[:10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Absolutely love this place! I've been here many times and it never fails to impress me  the bff burger is unreal and the tsoynamis and cupcakes are amazing!! Everything is pretty much the best\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some of the main steps outline a typical workflow for for text mining, assuming we have our dataset already downloaded and ready to be used:\n",
    "\n",
    "1. Text normalization\n",
    "2. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "Text normalization is a process that consists of a series of steps that should be followed to wrangle, clean, and standardize textual data into a form that could be consumed by other NLP and analytics systems and applications as input. \n",
    "\n",
    "Tokenization is a process of segmenting text into meaningful units. For example, \n",
    "\n",
    "For O'Neill, which of the following is the desired tokenization: neill, oneill, o'neill, (o')(neill), (o)(neill)?\n",
    "\n",
    "And for aren't: aren't, arent, (are)(n't), (aren)(t)?\n",
    "\n",
    "Besides tokenization, other techniques include:\n",
    "\n",
    "1. Removing special characters (text cleaning)\n",
    "2. Case conversion\n",
    "3. Correcting spellings\n",
    "4. Removing stopwords and other unnecessary terms\n",
    "5. Lemmatization\n",
    "\n",
    "Text normalization is also often called *text cleansing or wrangling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "# You can install nltk by yourself, but it will disappear if the instance is shutdown\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dictionaty of stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # tockenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tokenizing Text\n",
    "\n",
    "\n",
    "Sentence tokenization is the process of splitting a text corpus into sentences that act as the first level of tokens which the corpus is comprised of. This is also known as sentence segmentation, because we try to segment the text into meaningful sentences. Any text corpus is a body of text where each paragraph comprises several sentences.\n",
    "\n",
    "We will use the `nltk` framework, which provides various interfaces for performing sentence tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand contractions\n",
    "\n",
    "Contractions are shortened version of words or syllables. They exist in either written or spoken forms. Shortened versions of existing words are created by removing specific letters and sounds. In case of English contractions, they are often created by removing one of the vowels from the word. \n",
    "\n",
    "A vocabulary for contractions and their corresponding expanded forms that you can access in the file `contractions.py` in a Python dictionary (which we again download from the textbook repo).\n",
    "\n",
    "Contraction map is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ain't\": 'is not',\n",
       " \"aren't\": 'are not',\n",
       " \"can't\": 'cannot',\n",
       " \"can't've\": 'cannot have',\n",
       " \"'cause\": 'because',\n",
       " \"could've\": 'could have',\n",
       " \"couldn't\": 'could not',\n",
       " \"couldn't've\": 'could not have',\n",
       " \"didn't\": 'did not',\n",
       " \"doesn't\": 'does not',\n",
       " \"don't\": 'do not',\n",
       " \"hadn't\": 'had not',\n",
       " \"hadn't've\": 'had not have',\n",
       " \"hasn't\": 'has not',\n",
       " \"haven't\": 'have not',\n",
       " \"he'd\": 'he would',\n",
       " \"he'd've\": 'he would have',\n",
       " \"he'll\": 'he will',\n",
       " \"he'll've\": 'he he will have',\n",
       " \"he's\": 'he is',\n",
       " \"how'd\": 'how did',\n",
       " \"how'd'y\": 'how do you',\n",
       " \"how'll\": 'how will',\n",
       " \"how's\": 'how is',\n",
       " \"I'd\": 'I would',\n",
       " \"I'd've\": 'I would have',\n",
       " \"I'll\": 'I will',\n",
       " \"I'll've\": 'I will have',\n",
       " \"I'm\": 'I am',\n",
       " \"I've\": 'I have',\n",
       " \"i'd\": 'i would',\n",
       " \"i'd've\": 'i would have',\n",
       " \"i'll\": 'i will',\n",
       " \"i'll've\": 'i will have',\n",
       " \"i'm\": 'i am',\n",
       " \"i've\": 'i have',\n",
       " \"isn't\": 'is not',\n",
       " \"it'd\": 'it would',\n",
       " \"it'd've\": 'it would have',\n",
       " \"it'll\": 'it will',\n",
       " \"it'll've\": 'it will have',\n",
       " \"it's\": 'it is',\n",
       " \"let's\": 'let us',\n",
       " \"ma'am\": 'madam',\n",
       " \"mayn't\": 'may not',\n",
       " \"might've\": 'might have',\n",
       " \"mightn't\": 'might not',\n",
       " \"mightn't've\": 'might not have',\n",
       " \"must've\": 'must have',\n",
       " \"mustn't\": 'must not',\n",
       " \"mustn't've\": 'must not have',\n",
       " \"needn't\": 'need not',\n",
       " \"needn't've\": 'need not have',\n",
       " \"o'clock\": 'of the clock',\n",
       " \"oughtn't\": 'ought not',\n",
       " \"oughtn't've\": 'ought not have',\n",
       " \"shan't\": 'shall not',\n",
       " \"sha'n't\": 'shall not',\n",
       " \"shan't've\": 'shall not have',\n",
       " \"she'd\": 'she would',\n",
       " \"she'd've\": 'she would have',\n",
       " \"she'll\": 'she will',\n",
       " \"she'll've\": 'she will have',\n",
       " \"she's\": 'she is',\n",
       " \"should've\": 'should have',\n",
       " \"shouldn't\": 'should not',\n",
       " \"shouldn't've\": 'should not have',\n",
       " \"so've\": 'so have',\n",
       " \"so's\": 'so as',\n",
       " \"that'd\": 'that would',\n",
       " \"that'd've\": 'that would have',\n",
       " \"that's\": 'that is',\n",
       " \"there'd\": 'there would',\n",
       " \"there'd've\": 'there would have',\n",
       " \"there's\": 'there is',\n",
       " \"they'd\": 'they would',\n",
       " \"they'd've\": 'they would have',\n",
       " \"they'll\": 'they will',\n",
       " \"they'll've\": 'they will have',\n",
       " \"they're\": 'they are',\n",
       " \"they've\": 'they have',\n",
       " \"to've\": 'to have',\n",
       " \"wasn't\": 'was not',\n",
       " \"we'd\": 'we would',\n",
       " \"we'd've\": 'we would have',\n",
       " \"we'll\": 'we will',\n",
       " \"we'll've\": 'we will have',\n",
       " \"we're\": 'we are',\n",
       " \"we've\": 'we have',\n",
       " \"weren't\": 'were not',\n",
       " \"what'll\": 'what will',\n",
       " \"what'll've\": 'what will have',\n",
       " \"what're\": 'what are',\n",
       " \"what's\": 'what is',\n",
       " \"what've\": 'what have',\n",
       " \"when's\": 'when is',\n",
       " \"when've\": 'when have',\n",
       " \"where'd\": 'where did',\n",
       " \"where's\": 'where is',\n",
       " \"where've\": 'where have',\n",
       " \"who'll\": 'who will',\n",
       " \"who'll've\": 'who will have',\n",
       " \"who's\": 'who is',\n",
       " \"who've\": 'who have',\n",
       " \"why's\": 'why is',\n",
       " \"why've\": 'why have',\n",
       " \"will've\": 'will have',\n",
       " \"won't\": 'will not',\n",
       " \"won't've\": 'will not have',\n",
       " \"would've\": 'would have',\n",
       " \"wouldn't\": 'would not',\n",
       " \"wouldn't've\": 'would not have',\n",
       " \"y'all\": 'you all',\n",
       " \"y'all'd\": 'you all would',\n",
       " \"y'all'd've\": 'you all would have',\n",
       " \"y'all're\": 'you all are',\n",
       " \"y'all've\": 'you all have',\n",
       " \"you'd\": 'you would',\n",
       " \"you'd've\": 'you would have',\n",
       " \"you'll\": 'you will',\n",
       " \"you'll've\": 'you will have',\n",
       " \"you're\": 'you are',\n",
       " \"you've\": 'you have'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! wget -nc -nv https://raw.githubusercontent.com/dipanjanS/text-analytics-with-python/master/Old-First-Edition/source_code/Ch04_Text_Classification/contractions.py\n",
    "from contractions import CONTRACTION_MAP\n",
    "CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        # not sure why below is there\n",
    "        # expanded_contraction = first_char+expanded_contraction[1:] \n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could have gone, but did not go'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"I could've gone, but didn't go\", CONTRACTION_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "text = [expand_contractions(sentence, CONTRACTION_MAP) for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Absolutely love this place! I have been here many times and it never fails to impress me  the bff burger is unreal and the tsoynamis and cupcakes are amazing!! Everything is pretty much the best'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Special Characters\n",
    "\n",
    "One important task in text normalization involves removing unnecessary and special characters. These may be special symbols or even punctuation that occurs in sentences. This step is often performed before or after tokenization. The main reason for doing so is because often punctuation or special characters do not have much significance when we analyze the text and utilize it for extracting features or information based on NLP and ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [remove_special_characters(sentence) for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Absolutely love this place I have been here many times and it never fails to impress me the bff burger is unreal and the tsoynamis and cupcakes are amazing Everything is pretty much the best'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "Stopwords are words that have little or no significance. They are usually removed from text during processing so as to retain words having maximum significance and context. Stopwords are usually words that end up occurring the most if you aggregated any corpus of text based on singular tokens and checked their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stopword_list):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [remove_stopwords(sentence,\n",
    "                         stopword_list=list(ENGLISH_STOP_WORDS))\\\n",
    "        for sentence in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Parts of Speech and Lemmatization\n",
    "\n",
    "Now that we have a function for expanding contractions, we implement a function for standardizing our text data by bringing word tokens to their base or root form using lemmatization. For example, \n",
    "\n",
    "* better to good (lemmatization but not stemming would get correct)\n",
    "* walking to walk (lemmatization and stemming would both yield same result)\n",
    "* meeting? (part of speech is needed for correctly normalizing this text)\n",
    "\n",
    "The following functions will help us in achieving that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['averaged_perceptron_tagger',\n",
    "               'universal_tagset',\n",
    "               'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog jumps'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('brown', 'ADJ'),\n",
       " ('fox', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('quick', 'ADJ'),\n",
       " ('and', 'CONJ'),\n",
       " ('he', 'PRON'),\n",
       " ('is', 'VERB'),\n",
       " ('jumping', 'VERB'),\n",
       " ('over', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('lazy', 'ADJ'),\n",
       " ('dog', 'NOUN'),\n",
       " ('jumps', 'NOUN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    tagged_text = pos_tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "- The process of lemmatization is very similar to stemming; you remove word affixes to get to a base form of the word. \n",
    "\n",
    "- But in this case, this base form is also known as the root word, but not the root stem. \n",
    "\n",
    "- The difference is that the root stem may not always be a lexicographically correct word; that is, it may not be present in the dictionary. The root word, also known as the lemma, will always be present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize text based on POS tags    \n",
    "def lemmatize_text(text):\n",
    "    text = tokenize_text(text)\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenize_text(sentence) for sentence in text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [tokenize_text(tokens) for tokens in text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization pipeline\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_text(text,tokenize=False):\n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = text.lower()\n",
    "    text = remove_stopwords(text,ENGLISH_STOP_WORDS)\n",
    "    text = keep_text_characters(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Absolutely love this place! I've been here many times and it never fails to impress me  the bff burger is unreal and the tsoynamis and cupcakes are amazing!! Everything is pretty much the best\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absolutely love place time fail impress bff burger unreal tsoynamis cupcake amaze pretty best'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(review_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89249"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time consuming (processed file is provided for download)\n",
    "# review_corpus_norm = [normalize_text(text) for text in review_corpus]\n",
    "# pickle.dump(review_corpus_norm,open('review_corpus_norm.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "- In ML terminology, features are unique, measurable attributes or properties for each observation or data point in a dataset. Features are usually numeric in nature and can be absolute numeric values or categorical.\n",
    "- With textual data there is the added challenge of figuring out how to transform textual data and extract numeric features from it.\n",
    "\n",
    "## Vector Space Model\n",
    "\n",
    "Say we have a document $D$ in a document vector space VS. The number of dimensions or columns for each document will be the total number of distinct terms or words for all documents in the vector space. So, the vector space can be denoted\n",
    "$$\n",
    "VS:=\\{W_{1},\\ldots,W_{n}\\}\n",
    "$$\n",
    "where there are $n$ distinct words across all documents. Now we can represent document $D$ in this vector space as\n",
    "$$\n",
    "D:=\\{w_{D1},w_{D2},\\ldots,W_{Dn}\\},\n",
    "$$\n",
    "where $W_{Dn}$ denotes the weight for word $n$ in document $D$. \n",
    "\n",
    "Examples of feature-extraction techniques are:\n",
    "1. Bag of Words model\n",
    "2. TF-IDF model\n",
    "\n",
    "We will use of the `nltk`, `gensim`, and `scikit-learn` libraries,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words model\n",
    "\n",
    "Each document is converted into a vector that represents the frequency of all the distinct words that are present in the document vector space for that specific document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Model\n",
    "\n",
    "The Bag of Words model is good, but the vectors are completely based on absolute frequencies of word occurrences. This has some potential problems where words that may tend to occur a lot across all documents in the corpus will have higher frequencies and will tend to overshadow other words that may not occur as frequently but may be more interesting and effective as features to identify specific categories for the documents. \n",
    "\n",
    "TF-IDF stands for *Term Frequency-Inverse Document Frequency*, a combination of two metrics:\n",
    "\n",
    "(i) Term frequency, $tf$, is what we had computed in the Bag of Words model (i.e., raw frequency value of that term in a particular document).\n",
    "\n",
    "(ii) Inverse document frequency, $idf$, is the inverse of the document frequency for each term. It is computed by dividing the total number of documents in our corpus by the document frequency for each term and then applying logarithmic scaling on the result.\n",
    "$$\n",
    "idf_{ij}=1+\\log \\frac{C}{1+df_{i}}\n",
    "$$\n",
    "where $idf_{ij}$ represents the $idf$ for the $j$-th term in document $i$, $C$ represents the count of the total number of documents in our corpus, and $df_{i}$ represents the frequency of the number of documents in which the term $i$ is present.\n",
    "\n",
    "- The final TF-IDF metric we will be using is a normalized version of the tfidf matrix we get from the product of \n",
    "$$\n",
    "T_{ij}:=tf_{ij}\\times idf_{ij}.\n",
    "$$\n",
    "\n",
    "- Typically, the TF-IDF matrix, \n",
    "$$\n",
    "T:=\\{T_{1},\\ldots,T_{n}\\}\n",
    "$$\n",
    "is normalized by dividing each $T_{i}$ with respect to the $L_2$ norm of the matrix (defined as the square root of the sum of the square of each $T_{i}$). The final $tfidf$ feature vector is given by:\n",
    "$$\n",
    "T_{i}:= \\frac{T_{i}}{||T_{i}||_{2}^{2}}, i=1,\\ldots,n.\n",
    "$$\n",
    "where $||T_{i}||_{2}^{2}$ represents the Euclidean norm for the TF-IDF matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc -nv https://ucsb.box.com/shared/static/3jyrbfdo2mppmkprn2mnl6cuvmbquufn.pkl -O review_corpus_norm.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pickle\n",
    "review_corpus_norm = pickle.load(open('review_corpus_norm.pkl', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "import numpy as np\n",
    "from feature_extractors import tfidf_transformer\n",
    "from feature_extractors import bow_extractor    \n",
    "\n",
    "def tf_idf(corpus):\n",
    "    # Bag of words construction\n",
    "    bow_vectorizer, bow_features = bow_extractor(corpus=corpus)\n",
    "    # feature names\n",
    "    feature_names = bow_vectorizer.get_feature_names()\n",
    "    # TF-IDF    \n",
    "    tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "    tdidf_features = np.round(tdidf_features.todense(),2)\n",
    "    return((tdidf_features, feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdidf_features,feature_names = tf_idf(review_corpus_norm) # memory error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is not enought memory to process all selected review, let's restrict our attention to business from *California*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.reviews in CA: 2775 \n",
      "No.reviews in total: 89249\n"
     ]
    }
   ],
   "source": [
    "def check_state(business,state = \"CA\"):\n",
    "    flag = any([category in state for category in business['state']])\n",
    "    return(flag)\n",
    "\n",
    "# get business_id\n",
    "business_id_ca = [business['business_id'] for business in train_business \\\n",
    "       if check_state(business,\"CA\")]\n",
    "\n",
    "# check if bussiness is in CA\n",
    "reviews_ca = [review['business_id'] in business_id_ca \\\n",
    "              for review in train_review] \n",
    "\n",
    "# subset reviews\n",
    "review_corpus_norm_ca = [review for (review,cond) in \\\n",
    "                         zip(review_corpus_norm,reviews_ca) if cond] \n",
    "\n",
    "print(\"No.reviews in CA:\",len(review_corpus_norm_ca),\"\\n\"\n",
    "      \"No.reviews in total:\",len(review_corpus_norm))\n",
    "\n",
    "tdidf_features,feature_names = tf_idf(review_corpus_norm_ca) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "*Topic models* have been designed specifically for the purpose of extracting various distinguishing concepts or topics from a large corpus containing various types of documents.\n",
    "\n",
    "Topic modeling is a *unsupervised* learning technique since involves extracting features from document terms to generate clusters or groups of terms that are distinguishable from each other, and these cluster of words form topics or concepts. \n",
    "\n",
    "Some methods for topic discovery include:\n",
    "- Latent semantic indexing:  \n",
    "    Singular value decomposition or principal components analysis\n",
    "- Latent Dirichlet allocation:  \n",
    "    Assumes a document is a mixture of a small number of topics\n",
    "- Non-negative matrix factorization\n",
    "\n",
    "Scikit-learn has examples: http://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tdidf_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9034, 2775)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # (words, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-negative Matrix Factorization\n",
    "def non_negative_marix_decomp(n_components,train_data):\n",
    "    import sklearn.decomposition as skld\n",
    "    model = skld.NMF(n_components=n_components, \n",
    "                     init='nndsvda', max_iter=500, \n",
    "                     random_state=0)\n",
    "    W = model.fit_transform(train_data)\n",
    "    H = model.components_\n",
    "    nmf = (W,H)\n",
    "    return(nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time\n",
    "r = 10 # no. of topics\n",
    "W_topic10,H_topic10 = \\\n",
    "    non_negative_marix_decomp(n_components = r, train_data = X) \n",
    "\n",
    "H_topic10 /= H_topic10.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 16:09:26 URL:https://raw.githubusercontent.com/dipanjanS/text-analytics-with-python/ed5ea8068428fec37d1d06ec40cb9d64c6336d77/Old-First-Edition/source_code/Ch05_Text_Summarization/topic_modeling.py [9378/9378] -> \"topic_modeling.py\" [1]\r\n"
     ]
    }
   ],
   "source": [
    "! wget -nc -nv https://raw.githubusercontent.com/dipanjanS/text-analytics-with-python/ed5ea8068428fec37d1d06ec40cb9d64c6336d77/Old-First-Edition/source_code/Ch05_Text_Summarization/topic_modeling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Topic 0\n",
      "order 0.7899351550195463\n",
      "wait 0.7722701100649334\n",
      "line 0.49006202958208667\n",
      "table 0.45128712843377433\n",
      "minute 0.3883819479533293\n",
      "time 0.3032548400068957\n",
      "long 0.3025681685824335\n",
      "food 0.2688148864603481\n",
      "come 0.26248121011198555\n",
      "ask 0.23081931714244294\n",
      "\n",
      "\n",
      "# Topic 1\n",
      "crepe 0.8546272656989266\n",
      "jazz 0.7302639185926614\n",
      "live 0.6629334476862394\n",
      "montreal 0.6354272101078127\n",
      "music 0.5509932443950918\n",
      "old 0.4070818640070344\n",
      "garden 0.39074634327212915\n",
      "beautiful 0.3635234801206732\n",
      "band 0.34234537410365184\n",
      "nelson 0.33449256879312417\n",
      "\n",
      "\n",
      "# Topic 2\n",
      "vegan 1.3298307535206841\n",
      "pomegranate 0.5202653975113484\n",
      "healthy 0.4966531959234139\n",
      "delicious 0.47022463666783526\n",
      "vegetarian 0.4601005230037674\n",
      "raw 0.45925404611832604\n",
      "option 0.42257867920936437\n",
      "cafe 0.4047937002208041\n",
      "restaurant 0.3078140299115204\n",
      "free 0.29555057388917055\n",
      "\n",
      "\n",
      "# Topic 3\n",
      "salad 0.8487573314657838\n",
      "sandwich 0.8039721327551664\n",
      "pizza 0.7575668044802571\n",
      "try 0.4518912974896077\n",
      "burger 0.4478246677737318\n",
      "chicken 0.3712577058649714\n",
      "potato 0.30286440866298336\n",
      "cheese 0.262899491948161\n",
      "sweet 0.2595044217877004\n",
      "white 0.2564733428017403\n",
      "\n",
      "\n",
      "# Topic 4\n",
      "great 1.886692079271184\n",
      "food 0.5912760603392817\n",
      "service 0.48045442044318803\n",
      "awesome 0.2899530181209889\n",
      "staff 0.2747995177873379\n",
      "atmosphere 0.2516066913973517\n",
      "friendly 0.2332865864862168\n",
      "location 0.22606676910710835\n",
      "place 0.18532913396135894\n",
      "excellent 0.18171987527817043\n",
      "\n",
      "\n",
      "# Topic 5\n",
      "breakfast 1.8970379798955728\n",
      "lunch 0.4589973433036046\n",
      "dinner 0.45059247419164306\n",
      "egg 0.3359613357928064\n",
      "scramble 0.3267527219031414\n",
      "burrito 0.23154940986033679\n",
      "try 0.2294581192928876\n",
      "pancake 0.2238495663344517\n",
      "biscuit 0.22152091795891113\n",
      "potato 0.21234406232629824\n",
      "\n",
      "\n",
      "# Topic 6\n",
      "love 1.8417964567504879\n",
      "place 0.7078517003087121\n",
      "amazing 0.35216998830637447\n",
      "atmosphere 0.26396202111237793\n",
      "favorite 0.25943073399578503\n",
      "eat 0.17838774407645763\n",
      "food 0.15120728158751104\n",
      "absolutely 0.14441447623094947\n",
      "super 0.14152779772743812\n",
      "friendly 0.13341776068324035\n",
      "\n",
      "\n",
      "# Topic 7\n",
      "liberty 1.5336728460362168\n",
      "market 1.4234671558667729\n",
      "gilbert 0.5529874326680663\n",
      "downtown 0.3620933662641318\n",
      "restaurant 0.23930471152305294\n",
      "bathroom 0.17567464949853578\n",
      "joe 0.16316804831825263\n",
      "dinner 0.15305435807696177\n",
      "time 0.14347382339916193\n",
      "joes 0.13897065430729919\n",
      "\n",
      "\n",
      "# Topic 8\n",
      "good 1.29621091816238\n",
      "food 0.9560544258274444\n",
      "really 0.6908751232965982\n",
      "place 0.5827236798390212\n",
      "service 0.5273166789825937\n",
      "like 0.4452702224679615\n",
      "price 0.41457319372431\n",
      "nice 0.33273715619066074\n",
      "just 0.29428593348924464\n",
      "little 0.2910928523756903\n",
      "\n",
      "\n",
      "# Topic 9\n",
      "coffee 1.7509411125825505\n",
      "bar 0.6373293838102378\n",
      "espresso 0.23249256932724172\n",
      "drink 0.2021859223809221\n",
      "amaze 0.19340204747923753\n",
      "make 0.18252510693505175\n",
      "place 0.17863634805296238\n",
      "just 0.1701239431146602\n",
      "latte 0.16292062396986648\n",
      "awesome 0.16011601910607795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "num_topics = 10\n",
    "word_topic = W_topic10\n",
    "fontsize_base = 15# / np.max(word_topic) # font size for word with largest share in corpus\n",
    "\n",
    "for t in range(0, num_topics):\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = [feature_names[k] for k in top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "    print('# Topic', t)\n",
    "    for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "        print(word, share)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 246\n",
    "r = 5 # no. of topics\n",
    "W_topic5,H_topic5 = \\\n",
    "    non_negative_marix_decomp(n_components = r, train_data = X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Topic 0\n",
      "order 0.9198368014217333\n",
      "wait 0.8321917326089756\n",
      "food 0.559968957978148\n",
      "line 0.542957746366604\n",
      "table 0.5057148971558636\n",
      "minute 0.4212970503452989\n",
      "time 0.4156547665073546\n",
      "come 0.381500274611855\n",
      "long 0.35575769791599826\n",
      "ask 0.2735925995092175\n",
      "\n",
      "\n",
      "### Topic 1\n",
      "crepe 0.9053877928500326\n",
      "jazz 0.7599761892665806\n",
      "live 0.685061954523128\n",
      "montreal 0.6537869445757567\n",
      "music 0.5745900878570843\n",
      "old 0.4216773239705172\n",
      "garden 0.41295562202766073\n",
      "beautiful 0.3829151459631211\n",
      "band 0.35823806666232316\n",
      "nelson 0.3466619903031834\n",
      "\n",
      "\n",
      "### Topic 2\n",
      "vegan 1.2394724776261994\n",
      "healthy 0.5384361014156344\n",
      "delicious 0.5192384711299428\n",
      "pomegranate 0.5022451681381006\n",
      "vegetarian 0.4777156851389845\n",
      "raw 0.4407399074572619\n",
      "option 0.4173212392978555\n",
      "cafe 0.3981448726497877\n",
      "love 0.36597369815007297\n",
      "try 0.3642376219512937\n",
      "\n",
      "\n",
      "### Topic 3\n",
      "liberty 0.869068658819685\n",
      "market 0.7935744340980874\n",
      "breakfast 0.6752525592825409\n",
      "sandwich 0.5830132777946612\n",
      "pizza 0.5660926518515521\n",
      "salad 0.545509173178245\n",
      "try 0.4363376960089537\n",
      "coffee 0.3882260845387588\n",
      "good 0.37423612868830625\n",
      "dinner 0.37166704719405214\n",
      "\n",
      "\n",
      "### Topic 4\n",
      "great 1.4594724759443718\n",
      "food 0.9544835549425709\n",
      "love 0.8042445557164776\n",
      "place 0.7605156730857865\n",
      "service 0.6105880965552406\n",
      "atmosphere 0.4686197377696856\n",
      "staff 0.34873590518194064\n",
      "friendly 0.3335534549297646\n",
      "awesome 0.3175673610948413\n",
      "good 0.3079962377381972\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "num_topics = 5\n",
    "word_topic = W_topic5\n",
    "fontsize_base = 15# / np.max(word_topic) # font size for word with largest share in corpus\n",
    "\n",
    "for t in range(num_topics):\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = [feature_names[k] for k in top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "    print('### Topic', t)\n",
    "    for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "        print(word, share)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
